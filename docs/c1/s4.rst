.. _c1.4:


1.4 分组交换网络中的延迟、丢失和吞吐量
=================================================================

1.4 Delay, Loss, and Throughput in Packet-Switched Networks

.. tab:: 中文

.. tab:: 英文

Back in :ref:`Section 1.1 <c1.1>` we said that the Internet can be viewed as an infrastructure that provides services
to distributed applications running on end systems. Ideally, we would like Internet services to be able to
move as much data as we want between any two end systems, instantaneously, without any loss of
data. Alas, this is a lofty goal, one that is unachievable in reality. Instead, computer networks necessarily
constrain throughput (the amount of data per second that can be transferred) between end systems,
introduce delays between end systems, and can actually lose packets. On one hand, it is unfortunate
that the physical laws of reality introduce delay and loss as well as constrain throughput. On the other
hand, because computer networks have these problems, there are many fascinating issues surrounding
how to deal with the problems—more than enough issues to fill a course on computer networking and to
motivate thousands of PhD theses! In this section, we’ll begin to examine and quantify delay, loss, and
throughput in computer networks.

.. _c1.4.1:


1.4.1 分组交换网络中的延迟概述
-------------------------------------------------------

1.4.1 Overview of Delay in Packet-Switched Networks

.. tab:: 中文

.. tab:: 英文

Recall that a packet starts in a host (the source), passes through a series of routers, and ends its
journey in another host (the destination). As a packet travels from one node (host or router) to the
subsequent node (host or router) along this path, the packet suffers from several types of delays at each
node along the path. The most important of these delays are the **nodal processing delay**, **queuing
delay**, **transmission delay**, and **propagation delay**; together, these delays accumulate to give a **total
nodal delay**. The performance of many Internet applications—such as search, Web browsing, e-mail,
maps, instant messaging, and voice-over-IP—are greatly affected by network delays. In order to acquire
a deep understanding of packet switching and computer networks, we must understand the nature and
importance of these delays.


Types of Delay
~~~~~~~~~~~~~~~~~~~~~~

Let’s explore these delays in the context of :ref:`Figure 1.16 <Figure 1.16>`. As part of its end-to-end route between source
and destination, a packet is sent from the upstream node through router A to router B. Our goal is to
characterize the nodal delay at router A. Note that router A has an outbound link leading to router B.
This link is preceded by a queue (also known as a buffer). When the packet arrives at router A from the
upstream node, router A examines the packet’s header to determine the appropriate outbound link for
the packet and then directs the packet to this link. In this example, the outbound link for the packet is the
one that leads to router B. A packet can be transmitted on a link only if there is no other packet currently
being transmitted on the link and if there are no other packets preceding it in the queue; if the link is
currently busy or if there are other packets already queued for the link, the newly arriving packet will
then join the queue.

.. _Figure 1.16:

.. figure:: ../img/63-0.png
   :align: center
   :name: The nodal delay at router A

**Figure 1.16 The nodal delay at router A**


Processing Delay
~~~~~~~~~~~~~~~~~~

The time required to examine the packet’s header and determine where to direct the packet is part of
the **processing delay**. The processing delay can also include other factors, such as the time needed to
check for bit-level errors in the packet that occurred in transmitting the packet’s bits from the upstream
node to router A. Processing delays in high-speed routers are typically on the order of microseconds or
less. After this nodal processing, the router directs the packet to the queue that precedes the link to
router B. (In :ref:`Chapter 4 <c4>` we’ll study the details of how a router operates.)

Queuing Delay
~~~~~~~~~~~~~~~~~~


At the queue, the packet experiences a **queuing delay** as it waits to be transmitted onto the link. The
length of the queuing delay of a specific packet will depend on the number of earlier-arriving packets
that are queued and waiting for transmission onto the link. If the queue is empty and no other packet is
currently being transmitted, then our packet’s queuing delay will be zero. On the other hand, if the traffic
is heavy and many other packets are also waiting to be transmitted, the queuing delay will be long. We
will see shortly that the number of packets that an arriving packet might expect to find is a function of the
intensity and nature of the traffic arriving at the queue. ­Queuing delays can be on the order of
microseconds to milliseconds in practice.

Transmission Delay
~~~~~~~~~~~~~~~~~~~~~~

Assuming that packets are transmitted in a first-come-first-served manner, as is common in packet-
switched networks, our packet can be transmitted only after all the packets that have arrived before it
have been transmitted. Denote the length of the packet by L bits, and denote the transmission rate of
the link from router A to router B by R bits/sec. For example, for a 10 Mbps Ethernet link, the rate is
R=10 Mbps; for a 100 Mbps Ethernet link, the rate is R=100 Mbps. The transmission delay is L/R. This
is the amount of time required to push (that is, transmit) all of the packet’s bits into the link.
Transmission delays are typically on the order of microseconds to milliseconds in practice.

Propagation Delay
~~~~~~~~~~~~~~~~~~~~~~

Once a bit is pushed into the link, it needs to propagate to router B. The time required to propagate from
the beginning of the link to router B is the **propagation delay**. The bit propagates at the propagation
speed of the link. The propagation speed depends on the physical medium of the link (that is, fiber
optics, twisted-pair copper wire, and so on) and is in the range of

    2⋅108 meters/sec to 3⋅108 meters/sec

which is equal to, or a little less than, the speed of light. The propagation delay is the distance between
two routers divided by the propagation speed. That is, the propagation delay is d/s, where d is the
distance between router A and router B and s is the propagation speed of the link. Once the last bit of
the packet propagates to node B, it and all the preceding bits of the packet are stored in router B. The
whole process then continues with router B now performing the forwarding. In wide-area networks,
propagation delays are on the order of milliseconds.

Comparing Transmission and Propagation Delay
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. image:: ../img/64-0.png
   :align: center 
   :alt: Exploring propagation delay and transmission delay

Exploring propagation delay and transmission delay

Newcomers to the field of computer networking sometimes have difficulty understanding the difference
between transmission delay and propagation delay. The difference is subtle but important. The
transmission delay is the amount of time required for the router to push out the packet; it is a function of
the packet’s length and the transmission rate of the link, but has nothing to do with the distance between
the two routers. The propagation delay, on the other hand, is the time it takes a bit to propagate from
one router to the next; it is a function of the distance between the two routers, but has nothing to do with
the packet’s length or the transmission rate of the link.

An analogy might clarify the notions of transmission and propagation delay. Consider a highway that has
a tollbooth every 100 kilometers, as shown in :ref:`Figure 1.17 <Figure 1.17>`. You can think of the highway segments
between tollbooths as links and the tollbooths as routers. Suppose that cars travel (that is, propagate)
on the highway at a rate of 100 km/hour (that is, when a car leaves a tollbooth, it instantaneously
accelerates to 100 km/hour and maintains that speed between tollbooths). Suppose next that 10 cars,
traveling together as a caravan, follow each other in a fixed order. You can think of each car as a bit and
the caravan as a packet. Also suppose that each tollbooth services (that is, transmits) a car at a rate of one car per 12 seconds, and that it is late at night
so that the caravan’s cars are the only cars on the highway. Finally, suppose that whenever the first car
of the caravan arrives at a tollbooth, it waits at the entrance until the other nine cars have arrived and
lined up behind it. (Thus the entire caravan must be stored at the tollbooth before it can begin to be
forwarded.) The time required for the tollbooth to push the entire caravan onto the highway is
(10 cars)/(5 cars/minute)=2 minutes. This time is analogous to the transmission delay in a router. The
time required for a car to travel from the exit of one tollbooth to the next tollbooth is
100 km/(100 km/hour)=1 hour. This time is analogous to propagation delay. Therefore, the time from
when the caravan is stored in front of a tollbooth until the caravan is stored in front of the next tollbooth
is the sum of transmission delay and propagation delay—in this example, 62 minutes.

.. _Figure 1.17:

.. figure:: ../img/65-0.png
   :align: center
   :name: Caravan analogy

**Figure 1.17 Caravan analogy**

Let’s explore this analogy a bit more. What would happen if the tollbooth service time for a caravan were
greater than the time for a car to travel between tollbooths? For example, suppose now that the cars
travel at the rate of 1,000 km/hour and the tollbooth services cars at the rate of one car per minute. Then
the traveling delay between two tollbooths is 6 minutes and the time to serve a caravan is 10 minutes. In
this case, the first few cars in the caravan will arrive at the second tollbooth before the last cars in the
caravan leave the first tollbooth. This situation also arises in packet-switched networks—the first bits in a
packet can arrive at a router while many of the remaining bits in the packet are still waiting to be
transmitted by the preceding router.

If a picture speaks a thousand words, then an animation must speak a million words. The Web site for
this textbook provides an interactive Java applet that nicely illustrates and contrasts transmission delay
and propagation delay. The reader is highly encouraged to visit that applet. :ref:`[Smith 2009] <Smith 2009>` also provides
a very readable discussion of propagation, queueing, and transmission delays.

If we let :math:`d_{proc}`, :math:`d_{queue}`, :math:`d_{trans}`, and :math:`d_{prop}` denote the processing, queuing, transmission, and propagation delays, then the total nodal delay is given by

.. math::

   d_{nodal} = d_{proc} + d_{queue} + d_{trans} + d_{prop}

The contribution of these delay components can vary significantly. For example, :math:`d_{proc}` can be negligible
(for example, a couple of microseconds) for a link connecting two routers on the same university
campus; however, :math:`d_{proc}` is hundreds of milliseconds for two routers interconnected by a geostationary
satellite link, and can be the dominant term in :math:`d_{nodal}`. Similarly,  :math:`d_{trans}` can range from negligible to
significant. Its contribution is typically negligible for transmission rates of 10 Mbps and higher (for
example, for LANs); however, it can be hundreds of milliseconds for large Internet packets sent over
low-speed dial-up modem links. The processing delay, :math:`d_{prop}`, is often negligible; however, it strongly
influences a router’s maximum throughput, which is the maximum rate at which a router can forward
packets.

.. _c1.4.2:

1.4.2 排队延迟和数据包丢失
-------------------------------------------------------

1.4.2 Queuing Delay and Packet Loss

.. tab:: 中文

.. tab:: 英文

The most complicated and interesting component of nodal delay is the queuing delay, dqueue. In fact,
queuing delay is so important and interesting in computer networking that thousands of papers and
numerous books have been written about it [ :ref:`Bertsekas 1991 <Bertsekas 1991>`; :ref:`Daigle 1991 <Daigle 1991>`; :ref:`Kleinrock 1975 <Kleinrock 1975>`, :ref:`Kleinrock 1976 <Kleinrock 1976>`; :ref:`Ross 1995 <Ross 1995>`]. We give only a high-level, intuitive discussion of queuing delay here; the more
curious reader may want to browse through some of the books (or even eventually write a PhD thesis on
the subject!). Unlike the other three delays (namely, :math:`d_{proc}`, :math:`d_{trans}`, and :math:`d_{prop}`), the queuing delay can vary
from packet to packet. For example, if 10 packets arrive at an empty queue at the same time, the first
packet transmitted will suffer no queuing delay, while the last packet transmitted will suffer a relatively
large queuing delay (while it waits for the other nine packets to be transmitted). Therefore, when
characterizing queuing delay, one typically uses statistical measures, such as average queuing delay,
variance of queuing delay, and the probability that the queuing delay exceeds some specified value.

When is the queuing delay large and when is it insignificant? The answer to this question depends on
the rate at which traffic arrives at the queue, the transmission rate of the link, and the nature of the
arriving traffic, that is, whether the traffic arrives periodically or arrives in bursts. To gain some insight
here, let a denote the average rate at which packets arrive at the queue (a is in units of packets/sec).
Recall that R is the transmission rate; that is, it is the rate (in bits/sec) at which bits are pushed out of the
queue. Also suppose, for simplicity, that all packets consist of *L* bits. Then the average rate at which bits
arrive at the queue is *La* bits/sec. Finally, assume that the queue is very big, so that it can hold
essentially an infinite number of bits. The ratio *La/R*, called the traffic intensity, often plays an
important role in estimating the extent of the queuing delay. If *La/R* > 1, then the average rate at which
bits arrive at the queue exceeds the rate at which the bits can be transmitted from the queue. In this
unfortunate situation, the queue will tend to increase without bound and the queuing delay will approach
infinity! Therefore, one of the golden rules in traffic engineering is: *Design your system so that the traffic
intensity is no greater than 1.*

Now consider the case *La/R* ≤ 1. Here, the nature of the arriving traffic impacts the queuing delay. For
example, if packets arrive periodically—that is, one packet arrives every *L/R* seconds—then every
packet will arrive at an empty queue and there will be no queuing delay. On the other hand, if packets
arrive in bursts but periodically, there can be a significant average queuing delay. For example, suppose
N packets arrive simultaneously every (*L/R*)N seconds. Then the first packet transmitted has no queuing
delay; the second packet transmitted has a queuing delay of *L/R* seconds; and more generally, the nth
packet transmitted has a queuing delay of *(n−1)L/R* seconds. We leave it as an exercise for you to
calculate the average queuing delay in this example.

The two examples of periodic arrivals described above are a bit academic. ­Typically, the arrival
process to a queue is random; that is, the arrivals do not follow any pattern and the packets are spaced
apart by random amounts of time. In this more realistic case, the quantity La/R is not usually sufficient to
fully characterize the queuing delay statistics. Nonetheless, it is useful in gaining an intuitive
understanding of the extent of the queuing delay. In particular, if the traffic intensity is close to zero, then
packet arrivals are few and far between and it is unlikely that an arriving packet will find another packet
in the queue. Hence, the average queuing delay will be close to zero. On the other hand, when the
traffic intensity is close to 1, there will be intervals of time when the arrival rate exceeds the transmission
capacity (due to variations in packet arrival rate), and a queue will form during these periods of time;
when the arrival rate is less than the transmission capacity, the length of the queue will shrink.
Nonetheless, as the traffic intensity approaches 1, the average queue length gets larger and larger. The
qualitative dependence of average queuing delay on the traffic intensity is shown in :ref:`Figure 1.18 <Figure 1.18>`.

One important aspect of :ref:`Figure 1.18 <Figure 1.18>` is the fact that as the traffic intensity approaches 1, the average
queuing delay increases rapidly. A small percentage increase in the intensity will result in a much larger
percentage-wise increase in delay. Perhaps you have experienced this phenomenon on the highway. If
you regularly drive on a road that is typically congested, the fact that the road is typically
congested means that its traffic intensity is close to 1. If some event causes an even slightly larger-than-
usual amount of traffic, the delays you experience can be huge.

.. _Figure 1.18:

.. figure:: ../img/68-0.png
   :align: center
   :name: Dependence of average queuing delay on traffic intensity

**Figure 1.18 Dependence of average queuing delay on traffic intensity**

To really get a good feel for what queuing delays are about, you are encouraged once again to visit the
textbook Web site, which provides an interactive Java applet for a queue. If you set the packet arrival
rate high enough so that the traffic intensity exceeds 1, you will see the queue slowly build up over time.

Packet Loss
~~~~~~~~~~~~~~~~

In our discussions above, we have assumed that the queue is capable of holding an infinite number of
packets. In reality a queue preceding a link has finite capacity, although the queuing capacity greatly
depends on the router design and cost. Because the queue capacity is finite, packet delays do not really
approach infinity as the traffic intensity approaches 1. Instead, a packet can arrive to find a full queue.
With no place to store such a packet, a router will **drop** that packet; that is, the packet will be **lost**. This
overflow at a queue can again be seen in the Java applet for a queue when the traffic intensity is greater
than 1.

From an end-system viewpoint, a packet loss will look like a packet having been transmitted into the
network core but never emerging from the network at the destination. The fraction of lost packets
increases as the traffic intensity increases. Therefore, performance at a node is often measured not only
in terms of delay, but also in terms of the probability of packet loss. As we’ll discuss in the subsequent
chapters, a lost packet may be retransmitted on an end-to-end basis in order to ensure that all data are
eventually transferred from source to destination.

.. _c1.4.3:

1.4.3 端到端延迟
-------------------------------------------------------

1.4.3 End-to-End Delay

.. tab:: 中文

.. tab:: 英文


Our discussion up to this point has focused on the nodal delay, that is, the delay at a single router. Let’s
now consider the total delay from source to destination. To get a handle on this concept, suppose there
are N−1 routers between the source host and the destination host. Let’s also suppose for the moment
that the network is uncongested (so that queuing delays are negligible), the processing delay at each
router and at the source host is :math:`d_{proc}`, the transmission rate out of each router and out of the source host
is *R* bits/sec, and the propagation on each link is :math:`d_{prop}`. The nodal delays accumulate and give an end-to-
end delay,

.. _equation 1.2:

.. math::

    d_{end} − end = N(d_{proc}+d_{trans}+d_{prop}) \space\space\space\space\space\space\space\space\space\space (1.2)

where, once again, :math:`d_{trans}` =L/R , where L is the packet size. Note that :ref:`Equation 1.2 <Equation 1.2>` is a generalization of
:ref:`Equation 1.1 <Equation 1.1>`, which did not take into account processing and propagation delays. We leave it to you to
generalize :ref:`Equation 1.2 <Equation 1.2>` to the case of ­heterogeneous delays at the nodes and to the presence of an
average queuing delay at each node.

Traceroute
~~~~~~~~~~~~~~~~~

.. image:: ../img/69-0.png
   :align: center

**Using Traceroute to discover network paths and measure network delay**

To get a hands-on feel for end-to-end delay in a computer network, we can make use of the Traceroute
program. Traceroute is a simple program that can run in any Internet host. When the user specifies a
destination hostname, the program in the source host sends multiple, special packets toward that
destination. As these packets work their way toward the destination, they pass through a series of
routers. When a router receives one of these special packets, it sends back to the source a short
message that contains the name and address of the router.

More specifically, suppose there are *N−1* routers between the source and the destination. Then the
source will send *N* special packets into the network, with each packet addressed to the ultimate
destination. These *N* special packets are marked *1* through *N*, with the first packet marked 1 and the last
packet marked *N*. When the nth router receives the nth packet marked n, the router does not forward
the packet toward its destination, but instead sends a message back to the source. When the
destination host receives the Nth packet, it too returns a message back to the source. The source
records the time that elapses between when it sends a packet and when it receives the corresponding
return message; it also records the name and address of the router (or the destination host) that returns
the message. In this manner, the source can reconstruct the route taken by packets flowing from source
to destination, and the source can determine the round-trip delays to all the intervening routers.
Traceroute actually repeats the experiment just described three times, so the source actually sends *3 •
N* packets to the destination. RFC 1393 describes Traceroute in detail.

Here is an example of the output of the Traceroute program, where the route was being traced from the
source host `gaia.cs.umass.edu <http://gaia.cs.umass.edu/>`_ (at the University of ­Massachusetts) to the host `cis.poly.edu <http://cis.poly.edu/>`_ (at Polytechnic University in Brooklyn). The output has six columns: the first column is the n value
described above, that is, the number of the router along the route; the second column is the name of the
router; the third column is the address of the router (of the form xxx.xxx.xxx.xxx); the last three columns
are the round-trip delays for three experiments. If the source receives fewer than three messages from
any given router (due to packet loss in the network), Traceroute places an asterisk just after the router
number and reports fewer than three round-trip times for that router.

.. code-block:: text

    1  cs-gw (128.119.240.254) 1.009 ms 0.899 ms 0.993 ms
    2  128.119.3.154 (128.119.3.154) 0.931 ms 0.441 ms 0.651 ms
    3  -border4-rt-gi-1-3.gw.umass.edu (128.119.2.194) 1.032 ms 0.484 ms 0.451 ms
    4  -acr1-ge-2-1-0.Boston.cw.net (208.172.51.129) 10.006 ms 8.150 ms 8.460 ms
    5  -agr4-loopback.NewYork.cw.net (206.24.194.104) 12.272 ms 14.344 ms 13.267 ms
    6  -acr2-loopback.NewYork.cw.net (206.24.194.62) 13.225 ms 12.292 ms 12.148 ms
    7  -pos10-2.core2.NewYork1.Level3.net (209.244.160.133) 12.218 ms 11.823 ms 11.793 ms
    8  -gige9-1-52.hsipaccess1.NewYork1.Level3.net (64.159.17.39) 13.081 ms 11.556 ms 13.297 ms
    9  -p0-0.polyu.bbnplanet.net (4.25.109.122) 12.716 ms 13.052 ms 12.786 ms
    10 cis.poly.edu (128.238.32.126) 14.080 ms 13.035 ms 12.802 ms

In the trace above there are nine routers between the source and the destination. Most of these routers
have a name, and all of them have addresses. For example, the name of Router 3 is `border4-rt-gi-1-3.gw.umass.edu` and its address is `128.119.2.194` . Looking at the data provided for this same
router, we see that in the first of the three trials the round-trip delay between the source and the router
was 1.03 msec. The round-trip delays for the subsequent two trials were 0.48 and 0.45 msec. These
round-trip delays include all of the delays just discussed, including transmission delays, propagation
delays, router processing delays, and queuing delays. Because the queuing delay is varying with time,
the round-trip delay of packet *n* sent to a router n can sometimes be longer than the round-trip delay of
packet *n+1* sent to router *n+1*. Indeed, we observe this phenomenon in the above example: the delays
to Router 6 are larger than the delays to Router 7!

Want to try out Traceroute for yourself? We highly recommended that you visit http://www.traceroute.org, which provides a Web interface to an extensive list of sources for route tracing.
You choose a source and supply the hostname for any destination. The Traceroute program then does
all the work. There are a number of free software programs that provide a graphical interface to
Traceroute; one of our favorites is PingPlotter [ :ref:`PingPlotter 2016 <PingPlotter 2016>` ].

End System, Application, and Other Delays
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In addition to processing, transmission, and propagation delays, there can be additional significant
delays in the end systems. For example, an end system wanting to transmit a packet into a shared
medium (e.g., as in a WiFi or cable modem scenario) may purposefully delay its transmission as part of
its protocol for sharing the medium with other end systems; we’ll consider such protocols in detail in
Chapter 6. Another important delay is media packetization delay, which is present in Voice-over-IP
(VoIP) applications. In VoIP, the sending side must first fill a packet with encoded digitized speech
before passing the packet to the Internet. This time to fill a packet—called the packetization delay—can
be significant and can impact the user-perceived quality of a VoIP call. This issue will be further
explored in a homework problem at the end of this chapter.

.. _c1.4.4:

1.4.4 计算机网络中的吞吐量
-------------------------------------------------------

1.4.4 Throughput in Computer Networks

.. tab:: 中文

.. tab:: 英文


In addition to delay and packet loss, another critical performance measure in computer networks is end-
to-end throughput. To define throughput, consider transferring a large file from Host A to Host B across
a computer network. This transfer might be, for example, a large video clip from one peer to another in a
P2P file sharing system. The **instantaneous throughput** at any instant of time is the rate (in bits/sec) at
which Host B is receiving the file. (Many applications, including many P2P file sharing ­systems, display
the instantaneous throughput during downloads in the user interface—perhaps you have observed this
before!) If the file consists of F bits and the transfer takes T seconds for Host B to receive all F bits, then
the **average throughput** of the file transfer is F/T bits/sec. For some applications, such as Internet
telephony, it is desirable to have a low delay and an instantaneous throughput consistently above some
threshold (for example, over 24 kbps for some Internet telephony applications and over 256 kbps for
some real-time video applications). For other applications, including those involving file transfers, delay
is not critical, but it is desirable to have the highest possible throughput.

To gain further insight into the important concept of throughput, let’s consider a few examples. :ref:`Figure 1.19(a) <Figure 1.19>` shows two end systems, a server and a client, connected by two communication links and a
router. Consider the throughput for a file transfer from the server to the client. Let :math:`R_s` denote the rate of
the link between the server and the router; and :math:`R_c` denote the rate of the link between the router and the
client. Suppose that the only bits being sent in the entire network are those from the server to the client.
We now ask, in this ideal scenario, what is the server-to-client throughput? To answer this question, we
may think of bits as *fluid* and communication links as *pipes*. Clearly, the server cannot pump bits through
its link at a rate faster than :math:`R_s` bps; and the router cannot forward bits at a rate faster than :math:`R_c` bps. If
Rs<Rc, then the bits pumped by the server will “flow” right through the router and arrive at the client at a
rate of :math:`R_s` bps, giving a throughput of :math:`R_s` bps. If, on the other hand, Rc<Rs, then the router will not be
able to forward bits as quickly as it receives them. In this case, bits will only leave the router at rate :math:`R_c`,
giving an end-to-end throughput of :math:`R_c`. (Note also that if bits continue to arrive at the router at rate :math:`R_s`,
and continue to leave the router at :math:`R_c`, the backlog of bits at the router waiting
for transmission to the client will grow and grow—a most undesirable situation!) Thus, for this simple
two-link network, the throughput is :math:`min\{R_c, R_s\}`, that is, it is the transmission rate of the **bottleneck link**.
Having determined the throughput, we can now approximate the time it takes to transfer a large file of F
bits from server to client as :math:`F/min\{R_c, R_s\}`. For a specific example, suppose you are downloading an MP3
file of F=32 million bits, the server has a transmission rate of Rs=2 Mbps, and you have an access link
of Rc=1 Mbps. The time needed to transfer the file is then 32 seconds. Of course, these expressions for
throughput and transfer time are only approximations, as they do not account for store-and-forward and
processing delays as well as protocol issues.

.. _Figure 1.19:

.. figure:: ../img/72-0.png
   :align: center
   :name: Throughput for a file transfer from server to client

**Figure 1.19 Throughput for a file transfer from server to client**

:ref:`Figure 1.19(b) <Figure 1.19>` now shows a network with N links between the server and the client, with the
transmission rates of the *N* links being :math:`R1,R2,…, RN`. Applying the same analysis as for the two-link
network, we find that the throughput for a file transfer from server to client is :math:`min\{R1,R2,…, RN\}`, which
is once again the transmission rate of the bottleneck link along the path between server and client.

Now consider another example motivated by today’s Internet. :math:`Figure 1.20(a) <Figure 1.20>` shows two end systems, a
server and a client, connected to a computer network. Consider the throughput for a file transfer from
the server to the client. The server is connected to the network with an access link of rate :math:`R_s` and the
client is connected to the network with an access link of rate :math:`R_c`. Now suppose that all the links in the
core of the communication network have very high transmission rates, much higher than :math:`R_s` and :math:`R_c`.
Indeed, today, the core of the Internet is over-provisioned with high speed links that experience little
congestion. Also suppose that the only bits being sent in the entire network are those from the server to
the client. Because the core of the computer network is like a wide pipe in this example, the rate at
which bits can flow from source to destination is again the minimum of :math:`R_s` and :math:`R_c`, that is, throughput =
:math:`min\{R_s, R_c\}`. Therefore, the constraining factor for throughput in today’s Internet is typically the access
network.

For a final example, consider :ref:`Figure 1.20(b) <Figure 1.20>` in which there are 10 servers and 10 clients connected to
the core of the computer network. In this example, there are 10 simultaneous downloads taking place,
involving 10 client-server pairs. Suppose that these 10 downloads are the only traffic in the network at
the current time. As shown in the figure, there is a link in the core that is traversed by all 10 downloads.
Denote R for the transmission rate of this link R. Let’s suppose that all server access links have the
same rate :math:`R_s`, all client access links have the same rate :math:`R_c`, and the transmission rates of all the links in
the core—except the one common link of rate R—are much larger than :math:`R_s`, :math:`R_c`, and R. Now we ask, what
are the throughputs of the downloads? Clearly, if the rate of the common link, R, is large—say a
hundred times larger than both :math:`R_s` and Rc—then the throughput for each download will once again be
:math:`min\{R_s, R_c\}`. But what if the rate of the common link is of the same order as :math:`R_s` and :math:`R_c`? What will the
throughput be in this case? Let’s take a look at a specific example. Suppose Rs=2 Mbps, Rc=1 Mbps,
R=5 Mbps, and the common link divides its transmission rate equally among the 10 downloads. Then the bottleneck for
each download is no longer in the access network, but is now instead the shared link in the core, which
only provides each download with 500 kbps of throughput. Thus the end-to-end throughput for each
download is now reduced to 500 kbps.

.. _Figure 1.20:

.. figure:: ../img/74-0.png
   :align: left
   :name: Throughput for a file transfer from server to client

.. figure:: ../img/74-1.png
   :align: center
   :name: Throughput for a file transfer from server to client2

|

**Figure 1.20 End-to-end throughput: (a) Client downloads a file from ­server; (b) 10 clients downloading with 10 servers**

The examples in :ref:`Figure 1.19 <Figure 1.19>` and :ref:`Figure 1.20(a) <Figure 1.20>` show that throughput depends on the transmission
rates of the links over which the data flows. We saw that when there is no other intervening traffic, the
throughput can simply be approximated as the minimum transmission rate along the path between
source and destination. The example in :ref:`Figure 1.20(b) <Figure 1.20>` shows that more generally the throughput
depends not only on the transmission rates of the links along the path, but also on the intervening traffic.
In particular, a link with a high transmission rate may nonetheless be the bottleneck link for a file transfer
if many other data flows are also passing through that link. We will examine throughput in computer
networks more closely in the homework problems and in the subsequent chapters.